{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# X = (hours sleeping, hours studying), y = Score on test\n",
    "X = np.array(([3,5], [5,1], [10,2]), dtype=float)\n",
    "y = np.array(([75], [82], [93]), dtype=float)\n",
    "\n",
    "# Normalize\n",
    "X = X/np.amax(X, axis=0)\n",
    "y = y/100 #Max test score is 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.inputLayerSize = 2\n",
    "        self.outputLayerSize = 1\n",
    "        self.hiddenLayerSize = 3\n",
    "        \n",
    "        #initialize the random weights\n",
    "        self.W1 = np.random.randn(self.inputLayerSize, self.hiddenLayerSize)\n",
    "        self.W2 = np.random.randn(self.hiddenLayerSize, self.outputLayerSize)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        #propagates inputs through networks.\n",
    "        self.z2 = np.dot(X,self.W1)\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        self.z3 = np.dot(self.a2,self.W2)\n",
    "        yHat = self.sigmoid(self.z3)\n",
    "        return yHat\n",
    "        \n",
    "    def sigmoid(self,Z):\n",
    "        #apply the sigmoid activation function\n",
    "        return (1/(1+np.exp(-Z)))\n",
    "    \n",
    "    def sigmoidPrime(self,Z):\n",
    "        #apply the sigmoid activation derivation function\n",
    "        return (np.exp(-Z)/((1+np.exp(-Z))**2))\n",
    "    \n",
    "    def costFunction(self,X,y):\n",
    "        self.yHat = self.forward(X)\n",
    "        J = 0.5*sum((y-self.yHat)**2)\n",
    "        return J\n",
    "\n",
    "    def costFunctionPrime(self, X, y):\n",
    "        #Compute derivative with respect to W and W2 for a given X and y:\n",
    "        self.yHat = self.forward(X)\n",
    "        \n",
    "        delta3 = np.multiply(-(y-self.yHat), self.sigmoidPrime(self.z3))\n",
    "        dJdW2 = np.dot(self.a2.T, delta3)\n",
    "        \n",
    "        delta2 = np.dot(delta3, self.W2.T)*self.sigmoidPrime(self.z2)\n",
    "        dJdW1 = np.dot(X.T, delta2)  \n",
    "        \n",
    "        return dJdW1, dJdW2\n",
    "    \n",
    "    def getParams(self):\n",
    "        params = np.concatenate((self.W1.ravel(),self.W2.ravel()))\n",
    "        return params\n",
    "    \n",
    "    def setParams(self, params):\n",
    "        #Set W1 and W2 using single paramater vector.\n",
    "        W1_start = 0\n",
    "        W1_end = self.hiddenLayerSize * self.inputLayerSize\n",
    "        self.W1 = np.reshape(params[W1_start:W1_end], (self.inputLayerSize , self.hiddenLayerSize))\n",
    "        W2_end = W1_end + self.hiddenLayerSize*self.outputLayerSize\n",
    "        self.W2 = np.reshape(params[W1_end:W2_end], (self.hiddenLayerSize, self.outputLayerSize))\n",
    "        \n",
    "    def computeGradients(self,X,Y):\n",
    "        djdw1,djdw2 = self.costFunctionPrime(X,Y)\n",
    "        return np.concatenate((djdw1.ravel(),djdw2.ravel()))\n",
    "\n",
    "def computeNumericalGradient(N,X,Y):\n",
    "    params = N.getParams()\n",
    "    numgrad = np.zeros(params.shape)\n",
    "    perturb = np.zeros(params.shape)\n",
    "    e = 1e-4\n",
    "    print(params)\n",
    "    for p in range(len(params)):\n",
    "        perturb[p] = e            \n",
    "        N.setParams(params+perturb)\n",
    "        loss2 = N.costFunction(X,Y)\n",
    "        #print(type(loss2))\n",
    "        N.setParams(params-perturb)\n",
    "        loss1 = N.costFunction(X,Y)\n",
    "        \n",
    "        numgrad[p] = (loss2-loss1)/(2*e)\n",
    "        perturb[p] = 0\n",
    "    N.setParams(params)\n",
    "    return numgrad\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00266876,  0.03650722, -0.0373582 ,  0.00216782,  0.02967355,\n",
       "       -0.03335717, -0.18597606, -0.17562522, -0.16134461])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN = NeuralNetwork()\n",
    "grad = NN.computeGradients(x,y)\n",
    "grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.69821064 -0.08143537  1.17144758  0.0328924  -0.05837331 -0.31502344\n",
      "  1.67897696  2.3057788   0.59467548]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.00168613,  0.00214985,  0.00064884,  0.00564361,  0.00770778,\n",
       "        0.00203744,  0.00784743,  0.0081341 ,  0.00850076])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN = NeuralNetwork()\n",
    "numGrad = computeNumericalGradient(NN,x,y)\n",
    "numGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00667323,  0.04546069,  0.20898335])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN.costFunction(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sigmoid(np.random.randn(3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.01098352,  0.03332509,  0.03305924],\n",
       "        [ 0.00821445,  0.02884472,  0.02919704]]),\n",
       " array([[ 0.05907989,  0.04177399,  0.01030872],\n",
       "        [ 0.03339024,  0.02361718,  0.00584798],\n",
       "        [ 0.02866239,  0.0202368 ,  0.00491755]]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN.costFunctionPrime(x,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3/3 [==============================] - 0s - loss: 0.6039 - acc: 0.0000e+00\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 0s - loss: 0.6008 - acc: 0.0000e+00\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 0s - loss: 0.5976 - acc: 0.0000e+00\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 0s - loss: 0.5945 - acc: 0.0000e+00\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 0s - loss: 0.5913 - acc: 0.0000e+00\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 0s - loss: 0.5882 - acc: 0.0000e+00\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 0s - loss: 0.5850 - acc: 0.0000e+00\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 0s - loss: 0.5818 - acc: 0.0000e+00\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 0s - loss: 0.5787 - acc: 0.0000e+00\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 0s - loss: 0.5755 - acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.02088054]], dtype=float32)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "X = np.array(([5,2], [5,1], [5,3]), dtype=float)\n",
    "y = np.array(([75], [62], [93]), dtype=float)\n",
    "\n",
    "# Normalize\n",
    "X = X/np.amax(X, axis=0)\n",
    "y = y/100 #Max test score is 100\n",
    "model = Sequential()\n",
    "model.add(Dense(3, input_dim=2, activation='relu'))\n",
    "model.add(Dense(3,  activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam',metrics=['accuracy'])\n",
    "model.fit(X,y)\n",
    "model.predict(np.array([[10,2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n",
    "y = np.array([[0,1,1,0]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1],\n",
       "       [0, 1, 1],\n",
       "       [1, 0, 1],\n",
       "       [1, 1, 1]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "syn0 = np.random.randn(3,4)\n",
    "syn1 = np.random.randn(4,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.00304255 -0.13323203  0.05601549  0.07343647]\n",
      " [-0.4067525  -1.21630025 -0.86916172 -1.41188771]\n",
      " [ 0.2567302  -2.03621163  0.00615706  0.21373147]]\n",
      "[[-0.54676742]\n",
      " [-1.06529476]\n",
      " [-0.46787434]\n",
      " [-0.50851757]]\n"
     ]
    }
   ],
   "source": [
    "print(syn0)\n",
    "print(syn1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/surya/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  \n",
      "/home/surya/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in multiply\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    l1 = 1/1+(np.exp(np.dot(X,syn0)))\n",
    "    l2 = 1/1+(np.exp(np.dot(l1,syn1)))\n",
    "    l2_delta = (y-l2)*(l2*(1-l2))\n",
    "    l1_delta = l2_delta.dot(syn1.T)*(l1*(1-l1))\n",
    "    syn1 += l1.T.dot(l2_delta)\n",
    "    syn0 += X.T.dot(l1_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.00304255, -0.13323203,  0.05601549,  0.07343647],\n",
       "       [-0.4067525 , -1.21630025, -0.86916172, -1.41188771],\n",
       "       [ 0.2567302 , -2.03621163,  0.00615706,  0.21373147]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output After Training:\n",
      "[[ 0.00966449]\n",
      " [ 0.00786506]\n",
      " [ 0.99358898]\n",
      " [ 0.99211957]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# sigmoid function\n",
    "def nonlin(x,deriv=False):\n",
    "    if(deriv==True):\n",
    "        return x*(1-x)\n",
    "    return 1/(1+np.exp(-x))\n",
    "    \n",
    "# input dataset\n",
    "X = np.array([  [0,0,1],\n",
    "                [0,1,1],\n",
    "                [1,0,1],\n",
    "                [1,1,1] ])\n",
    "    \n",
    "# output dataset            \n",
    "y = np.array([[0,0,1,1]]).T\n",
    "\n",
    "# seed random numbers to make calculation\n",
    "# deterministic (just a good practice)\n",
    "np.random.seed(1)\n",
    "\n",
    "# initialize weights randomly with mean 0\n",
    "syn0 = 2*np.random.random((3,1)) - 1\n",
    "\n",
    "for iter in xrange(10000):\n",
    "\n",
    "    # forward propagation\n",
    "    l0 = X\n",
    "    l1 = nonlin(np.dot(l0,syn0))\n",
    "\n",
    "    # how much did we miss?\n",
    "    l1_error = y - l1\n",
    "\n",
    "    # multiply how much we missed by the \n",
    "    # slope of the sigmoid at the values in l1\n",
    "    l1_delta = l1_error * nonlin(l1,True)\n",
    "\n",
    "    # update weights\n",
    "    syn0 += np.dot(l0.T,l1_delta)\n",
    "\n",
    "print \"Output After Training:\"\n",
    "print l1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:0.496410031903\n",
      "Error:0.00858452565325\n",
      "Error:0.00578945986251\n",
      "Error:0.00462917677677\n",
      "Error:0.00395876528027\n",
      "Error:0.00351012256786\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def nonlin(x,deriv=False):\n",
    "\tif(deriv==True):\n",
    "\t    return x*(1-x)\n",
    "\n",
    "\treturn 1/(1+np.exp(-x))\n",
    "    \n",
    "X = np.array([[0,0,1],\n",
    "            [0,1,1],\n",
    "            [1,0,1],\n",
    "            [1,1,1]])\n",
    "                \n",
    "y = np.array([[0],\n",
    "\t\t\t[1],\n",
    "\t\t\t[1],\n",
    "\t\t\t[0]])\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "# randomly initialize our weights with mean 0\n",
    "syn0 = 2*np.random.random((3,4)) - 1\n",
    "syn1 = 2*np.random.random((4,1)) - 1\n",
    "\n",
    "for j in xrange(60000):\n",
    "\n",
    "\t# Feed forward through layers 0, 1, and 2\n",
    "    l0 = X\n",
    "    l1 = nonlin(np.dot(l0,syn0))\n",
    "    l2 = nonlin(np.dot(l1,syn1))\n",
    "\n",
    "    # how much did we miss the target value?\n",
    "    l2_error = y - l2\n",
    "    \n",
    "    if (j% 10000) == 0:\n",
    "        print \"Error:\" + str(np.mean(np.abs(l2_error)))\n",
    "        \n",
    "    # in what direction is the target value?\n",
    "    # were we really sure? if so, don't change too much.\n",
    "    l2_delta = l2_error*nonlin(l2,deriv=True)\n",
    "\n",
    "    # how much did each l1 value contribute to the l2 error (according to the weights)?\n",
    "    l1_error = l2_delta.dot(syn1.T)\n",
    "    \n",
    "    # in what direction is the target l1?\n",
    "    # were we really sure? if so, don't change too much.\n",
    "    l1_delta = l1_error * nonlin(l1,deriv=True)\n",
    "\n",
    "    syn1 += l1.T.dot(l2_delta)\n",
    "    syn0 += l0.T.dot(l1_delta)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
